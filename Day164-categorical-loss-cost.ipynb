{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"markdown","source":"Classification\n1.Binary Cross Entrophy / Log Loss : \nBinary cross-entropy (log loss) is a loss function used in binary classification problems. It quantifies the difference between the actual class labels (0 or 1) and the predicted probabilities output by the model. The lower the binary cross-entropy value, the better the modelâ€™s predictions align with the true labels.\n\n2. Categorical Cross Entrophy\nCategorical Cross-Entropy (CCE), also known as softmax loss or log loss ,  It measures the difference between the predicted probability distribution and the actual (true) distribution of classes. and used one-hot encoded.\n\n3.Sparse Categorial Entrophy : \nSparse Categorical Crossentropy is functionally similar to Categorical Crossentropy but is designed for cases where the target labels are not one-hot encoded.\n\n4.Hinge Loss :\nHinge loss is used in binary classification problems where the objective is to separate the data points in two classes typically labeled as +1 and -1.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
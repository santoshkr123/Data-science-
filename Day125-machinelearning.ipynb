{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"🔷 𝟭. 𝗥𝗶𝗱𝗴𝗲 𝗥𝗲𝗴𝗿𝗲𝘀𝘀𝗶𝗼𝗻 (𝗟𝟮 𝗥𝗲𝗴𝘂𝗹𝗮𝗿𝗶𝘇𝗮𝘁𝗶𝗼𝗻)\n𝗖𝗼𝗻𝗰𝗲𝗽𝘁: Adds 𝗟𝟮 𝗽𝗲𝗻𝗮𝗹𝘁𝘆 (sum of squared coefficients) to the cost function. It shrinks the coefficients but 𝗱𝗼𝗲𝘀𝗻’𝘁 𝗳𝗼𝗿𝗰𝗲 𝘁𝗵𝗲𝗺 𝘁𝗼 𝘇𝗲𝗿𝗼.\n𝗨𝘀𝗲 𝗖𝗮𝘀𝗲: When you have many correlated features. It reduces model complexity without feature elimination.\n✅ Keeps all features\n✅ Useful for multicollinearity\n✅ Controls overfitting\n❌ Doesn’t perform feature selection\n\n🔶 𝟮. 𝗟𝗮𝘀𝘀𝗼 𝗥𝗲𝗴𝗿𝗲𝘀𝘀𝗶𝗼𝗻 (𝗟𝟭 𝗥𝗲𝗴𝘂𝗹𝗮𝗿𝗶𝘇𝗮𝘁𝗶𝗼𝗻)\n𝗖𝗼𝗻𝗰𝗲𝗽𝘁: Adds 𝗟𝟭 𝗽𝗲𝗻𝗮𝗹𝘁𝘆 (sum of absolute values of coefficients) to the cost function. It can 𝘀𝗵𝗿𝗶𝗻𝗸 𝘀𝗼𝗺𝗲 𝗰𝗼𝗲𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁𝘀 𝘁𝗼 𝘇𝗲𝗿𝗼, effectively performing 𝗳𝗲𝗮𝘁𝘂𝗿𝗲 𝘀𝗲𝗹𝗲𝗰𝘁𝗶𝗼𝗻.\n✅ Performs feature selection\n✅ Useful for high-dimensional data\n❌ Can be unstable with highly correlated variables\n\n🟢 𝟯. 𝗘𝗹𝗮𝘀𝘁𝗶𝗰 𝗡𝗲𝘁 𝗥𝗲𝗴𝗿𝗲𝘀𝘀𝗶𝗼𝗻\nConcept: Combination of Lasso and Ridge. It includes both L1 and L2 penalties.\n𝗨𝘀𝗲 𝗖𝗮𝘀𝗲: Best when you have many features, some of which are correlated and irrelevant.\n✅ Balances feature selection and coefficient shrinkage\n✅ Works well for correlated predictors\n✅ Robust and flexible\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
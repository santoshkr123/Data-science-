{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 𝗖𝗼𝗻𝗳𝘂𝘀𝗶𝗼𝗻 𝗠𝗮𝘁𝗿𝗶𝘅 & 𝗘𝘃𝗮𝗹𝘂𝗮𝘁𝗶𝗼𝗻 𝗠𝗲𝘁𝗿𝗶𝗰𝘀","metadata":{}},{"cell_type":"markdown","source":"🧩 𝗖𝗼𝗻𝗳𝘂𝘀𝗶𝗼𝗻 𝗠𝗮𝘁𝗿𝗶𝘅\nA 𝗰𝗼𝗻𝗳𝘂𝘀𝗶𝗼𝗻 𝗺𝗮𝘁𝗿𝗶𝘅 is a summary table used to evaluate the performance of a classification model. It shows the number of:\n✅ 𝗧𝗿𝘂𝗲 𝗣𝗼𝘀𝗶𝘁𝗶𝘃𝗲𝘀 (𝗧𝗣): Correctly predicted positive cases\n❌ 𝗙𝗮𝗹𝘀𝗲 𝗣𝗼𝘀𝗶𝘁𝗶𝘃𝗲𝘀 (𝗙𝗣): Incorrectly predicted as positive\n❎ 𝗧𝗿𝘂𝗲 𝗡𝗲𝗴𝗮𝘁𝗶𝘃𝗲𝘀 (𝗧𝗡): Correctly predicted negative cases\n⚠️ 𝗙𝗮𝗹𝘀𝗲 𝗡𝗲𝗴𝗮𝘁𝗶𝘃𝗲𝘀 (𝗙𝗡): Incorrectly predicted as negative","metadata":{}},{"cell_type":"markdown","source":"✅ 𝗔𝗰𝗰𝘂𝗿𝗮𝗰𝘆\nThe ratio of correctly predicted observations (TP + TN) to the total observations.\nAccuracy=TP+TN / TP+FP+FN+TN\n✅ Great when classes are balanced\n⚠️ Misleading if the data is imbalanced.","metadata":{}},{"cell_type":"markdown","source":"🎯 𝗣𝗿𝗲𝗰𝗶𝘀𝗶𝗼𝗻\nOut of all predicted positives, how many are actually positive?\nPrecision = TP /TP +FP\n✅ Useful when 𝗳𝗮𝗹𝘀𝗲 𝗽𝗼𝘀𝗶𝘁𝗶𝘃𝗲𝘀 𝗮𝗿𝗲 𝗰𝗼𝘀𝘁𝗹𝘆 (e.g., spam detection).","metadata":{}},{"cell_type":"markdown","source":"📈 𝗥𝗲𝗰𝗮𝗹𝗹 (𝗦𝗲𝗻𝘀𝗶𝘁𝗶𝘃𝗶𝘁𝘆 𝗼𝗿 𝗧𝗣𝗥)\nOut of all actual positives, how many were correctly predicted?\nRecall = TP /TP+FN\n\n🧠 𝗙𝟭-𝗦𝗰𝗼𝗿𝗲\nThe harmonic mean of Precision and Recall.\nF 1=2⋅ Precision*Recall /Precision +Recall","metadata":{}},{"cell_type":"markdown","source":"𝗜𝗳 𝗙𝗣 𝗶𝘀 𝗺𝗼𝗿𝗲 𝗶𝗺𝗽𝗼𝗿𝘁𝗮𝗻𝘁 𝘁𝗵𝗮𝗻 𝗙𝗡 :\nB=\nF 0.5 Score = 1.25[P*R] / P+R\n\nIf FN is more important than PF :\nB=2\nF 2 Score = 5[P*R] / P+R","metadata":{}},{"cell_type":"markdown","source":"💡 Summary:\nChoose metrics based on the problem — not just accuracy. Understand what matters: precision or recall, and use F1 or F-beta accordingly.","metadata":{}}]}
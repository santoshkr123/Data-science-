{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"Ollama is a tool that lets you run large language models (LLMs) locally on your own computer instead of using them through the cloud.\n\nIn plain terms\n\nThink of Ollama as:\n\n‚ÄúDocker for AI models‚Äù ‚Äî it makes downloading, running, and managing AI models easy.\n\nWhat Ollama does\n\nüß† Runs AI models like LLaMA, Mistral, Gemma, Phi, etc.\n\nüíª Works locally (your laptop or server)\n\nüîí Keeps your data private (nothing sent to the internet)\n\n‚ö° Optimized for performance on CPUs and GPUs\n\nüß© Simple command-line interface + local API\n\nWhy people use Ollama\n\nPrivacy: Your prompts and data stay on your machine\n\nOffline use: No internet required after setup\n\nNo API costs: No per-token fees\n\nEasy setup: One command to run a model\n\nAfter installing Ollama, you can run a model like this:\n\nollama run llama3","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
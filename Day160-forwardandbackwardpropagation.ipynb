{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Forward and Backward Propagation","metadata":{}},{"cell_type":"markdown","source":"*Forward propagation : \n Forward propagation is the fundamental process in a neural network where input data passes through multiple layers to generate an output. It is the process by which input data passes through each layer of neural network to generate output. \n\n*Backward propagation :\n Its goal is to reduce the difference between the modelâ€™s predicted output and the actual output by adjusting the weights and biases in the network.\n\n*Multilayer Perceptron : \nMulti-layer perceptrons (MLP) is an artificial neural network that has 3 or more layers of perceptrons. These layers are- a single input layer, 1 or more hidden layers, and a single output layer of perceptrons.\n\n*Chain Rule :\n chain rule to propagate the error backwards through the network layer by layer, efficiently calculating gradients for weight updates.\n\n*Gradient Descent :\n Gradient Descent to update the parameters layer-by-layer, moving toward minimizing the loss function.\n\n","metadata":{"execution":{"iopub.status.busy":"2025-07-26T06:22:15.415438Z","iopub.execute_input":"2025-07-26T06:22:15.415764Z","iopub.status.idle":"2025-07-26T06:22:15.427457Z","shell.execute_reply.started":"2025-07-26T06:22:15.415740Z","shell.execute_reply":"2025-07-26T06:22:15.426140Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
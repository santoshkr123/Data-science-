{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 𝗖𝗿𝗼𝘀𝘀-𝗩𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻 𝗮𝗻𝗱 𝗜𝘁𝘀 𝗧𝘆𝗽𝗲𝘀 ","metadata":{}},{"cell_type":"markdown","source":"✅ 𝗪𝗵𝗮𝘁 𝗶𝘀 𝗖𝗿𝗼𝘀𝘀-𝗩𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻?\n𝗖𝗿𝗼𝘀𝘀-𝘃𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻 is a 𝗿𝗲𝘀𝗮𝗺𝗽𝗹𝗶𝗻𝗴 𝘁𝗲𝗰𝗵𝗻𝗶𝗾𝘂𝗲 used to 𝗲𝘃𝗮𝗹𝘂𝗮𝘁𝗲 𝗺𝗮𝗰𝗵𝗶𝗻𝗲 𝗹𝗲𝗮𝗿𝗻𝗶𝗻𝗴 𝗺𝗼𝗱𝗲𝗹𝘀 on a limited data sample.\nIt helps 𝗽𝗿𝗲𝘃𝗲𝗻𝘁 𝗼𝘃𝗲𝗿𝗳𝗶𝘁𝘁𝗶𝗻𝗴 and ensures the model generalizes well to unseen data.\n","metadata":{}},{"cell_type":"markdown","source":"🔁 𝗪𝗵𝘆 𝘂𝘀𝗲 𝗖𝗿𝗼𝘀𝘀-𝗩𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻?\nChecks model’s robustness\nReduces bias and variance\nImproves model reliability","metadata":{"execution":{"iopub.status.busy":"2025-06-27T03:49:16.537090Z","iopub.execute_input":"2025-06-27T03:49:16.537393Z","iopub.status.idle":"2025-06-27T03:49:16.549933Z","shell.execute_reply.started":"2025-06-27T03:49:16.537372Z","shell.execute_reply":"2025-06-27T03:49:16.548722Z"}}},{"cell_type":"markdown","source":"🔄 𝗧𝘆𝗽𝗲𝘀 𝗼𝗳 𝗖𝗿𝗼𝘀𝘀-𝗩𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻\n 𝗟𝗲𝗮𝘃𝗲-𝗢𝗻𝗲-𝗢𝘂𝘁 𝗖𝗿𝗼𝘀𝘀-𝗩𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻 (𝗟𝗢𝗢𝗖𝗩)\nk = number of data points\nTrain on all data except one, test on the excluded one\nRepeat for each point\n✅ High accuracy\n⚠️ Very time-consuming","metadata":{}},{"cell_type":"markdown","source":"𝗟𝗲𝗮𝘃𝗲-𝗣-𝗢𝘂𝘁 𝗖𝗿𝗼𝘀𝘀-𝗩𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻 (𝗟𝗣𝗢 𝗖𝗩)\nLPO is a cross-validation technique where P data points are left out for testing, and the model is trained on the rest.\n\n📌 If P = 1 → becomes 𝗟𝗲𝗮𝘃𝗲-𝗢𝗻𝗲-𝗢𝘂𝘁 𝗖𝗩 (𝗟𝗢𝗢𝗖𝗩)\n📌 Used for 𝗲𝘅𝗵𝗮𝘂𝘀𝘁𝗶𝘃𝗲 𝘃𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻 on small datasets\n\n🧠 𝗣𝗿𝗼𝘀:\nVery 𝗮𝗰𝗰𝘂𝗿𝗮𝘁𝗲 𝗮𝗻𝗱 𝗿𝗲𝗹𝗶𝗮𝗯𝗹𝗲\n⚠️ 𝗖𝗼𝗻𝘀:\nComputationally expensive (especially for large P or large datasets)\n","metadata":{}},{"cell_type":"markdown","source":"𝗞-𝗙𝗼𝗹𝗱 𝗖𝗿𝗼𝘀𝘀-𝗩𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻\nSplit dataset into 𝗸 𝗲𝗾𝘂𝗮𝗹 𝗽𝗮𝗿𝘁𝘀 (folds)\nTrain on k−1 folds and test on 1 fold\nRepeat k times\nFinal score = average of all k runs\n✅ Most commonly used","metadata":{}},{"cell_type":"markdown","source":"\n 𝗦𝘁𝗿𝗮𝘁𝗶𝗳𝗶𝗲𝗱 𝗞-𝗙𝗼𝗹𝗱\nLike K-Fold, but preserves class distribution\nIdeal for imbalanced classification problems\n","metadata":{}},{"cell_type":"markdown","source":" 𝗧𝗶𝗺𝗲 𝗦𝗲𝗿𝗶𝗲𝘀 𝗖𝗿𝗼𝘀𝘀-𝗩𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻\nUsed for time-based data\nTraining always precedes testing in time\n✅ Respects temporal order\n⚠️ Standard K-fold not suitable here\n\n🚀 𝗧𝗮𝗸𝗲𝗮𝘄𝗮𝘆:\nChoose the 𝗿𝗶𝗴𝗵𝘁 𝗰𝗿𝗼𝘀𝘀-𝘃𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻 based on your 𝗱𝗮𝘁𝗮 𝘁𝘆𝗽𝗲 and 𝗽𝗿𝗼𝗯𝗹𝗲𝗺 𝗻𝗮𝘁𝘂𝗿𝗲.\nUse 𝗦𝘁𝗿𝗮𝘁𝗶𝗳𝗶𝗲𝗱 𝗞-𝗙𝗼𝗹𝗱 for classification, and 𝗧𝗶𝗺𝗲 𝗦𝗲𝗿𝗶𝗲𝘀 𝗖𝗩 for time-based predictions.","metadata":{}},{"cell_type":"markdown","source":"\n🛠️ 𝗛𝘆𝗽𝗲𝗿𝗽𝗮𝗿𝗮𝗺𝗲𝘁𝗲𝗿 𝗧𝘂𝗻𝗶𝗻𝗴\nThe process of finding the 𝗯𝗲𝘀𝘁 𝗰𝗼𝗺𝗯𝗶𝗻𝗮𝘁𝗶𝗼𝗻 𝗼𝗳 𝗽𝗮𝗿𝗮𝗺𝗲𝘁𝗲𝗿𝘀 𝘁hat optimize a model’s performance.\n\n🔍 𝗖𝗼𝗺𝗺𝗼𝗻 𝗧𝘂𝗻𝗶𝗻𝗴 𝗧𝗲𝗰𝗵𝗻𝗶𝗾𝘂𝗲𝘀:\n1️⃣ 𝗚𝗿𝗶𝗱 𝗦𝗲𝗮𝗿𝗰𝗵\nExhaustive search over parameter grid\nSlow, but thorough\n\n2️⃣ 𝗥𝗮𝗻𝗱𝗼𝗺𝗶𝘇𝗲𝗱 𝗦𝗲𝗮𝗿𝗰𝗵\nRandomly samples combinations\nFaster, good for large search spaces","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}